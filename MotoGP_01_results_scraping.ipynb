{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MotoGP Regression Part 1 - Web Scraping  \n",
    "Ankur Vishwakarma  \n",
    "Metis Winter 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://css.motogp.com/w2015/img/logos/motogp.svg?version=a4b2faf82fe5880934375619ed3dac5aadb10ea007af6f4f770b682e0d16dbf6\" width=\"120\" align=\"left\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents  \n",
    "1. [Preliminary Work](#prelim)\n",
    "2. [Functions](#functions)\n",
    "3. [Scraping Race Results](#scraper)\n",
    "4. [Scraping Racetrack Information](#tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Preliminary Work <a name='prelim'></a>  \n",
    "The required modules are imported here. The columns that'll make up the eventual dataframe, years for which to scrape the data, and the base URL that will be added on to are all declared in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary modules\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for all the data we will be scraping in this notebook\n",
    "headers = ['Year','TRK','Track','Category','Session','Date','Track_Condition','Track_Temp','Air_Temp',\n",
    "           'Humidity','Position','Points','Rider_Number','Rider_Name','Nationality','Team_Name',\n",
    "           'Bike','Avg_Speed','Time']\n",
    "\n",
    "years = ['2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016', '2017']\n",
    "\n",
    "base_url = 'http://www.motogp.com/en/Results+Statistics/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Functions <a name='functions'></a>  \n",
    "This notebook relies on several functions to request, read, clean, and write data. These functions are separated into 3 main blocks and they are:\n",
    "1. The `soup_special` function returns a BeautifulSoup object for a provided URL. There are many points in the scraper when we use this functionality.\n",
    "2. Another 8 functions are used to return various datapoints and lists, such as track temperature, humidity, and which classes raced at a particular track. They also return 'n/a' if a value is not found in the soup.\n",
    "3. The `get_all_stats` function is used to return a list of dictionary about all the riders in a particular race. It also calls the functions in item 2 above to return an entire row of data that can be appended to the final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) function return a soup object\n",
    "def soup_special(url):\n",
    "    \"\"\"Returns a BeautifulSoup object for the provided url\"\"\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) functions to get various datapoints from a soup object\n",
    "def get_date(soup):\n",
    "    \"\"\" Returns the date of the race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='padbot5')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = ','.join(find.text.replace(',',' ').split()[-3:])\n",
    "    return r\n",
    "\n",
    "def get_tr_con(soup):\n",
    "    \"\"\" Returns the track condition during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather track_condition')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[2]\n",
    "    return r\n",
    "\n",
    "def get_tr_tmp(soup):\n",
    "    \"\"\" Returns the track temperature during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather ground')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_air_tmp(soup):\n",
    "    \"\"\" Returns the air temperature during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather air')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_humidity(soup):\n",
    "    \"\"\" Returns the track humidity during a race, or 'n/a' if \n",
    "        information does not exist in the provided soup \"\"\"\n",
    "    find = soup.find(class_='sprite_weather humidity')\n",
    "    if find is None:\n",
    "        r = 'n/a'\n",
    "    else:\n",
    "        r = find.findNext().text.split()[1]\n",
    "    return r\n",
    "\n",
    "def get_all_races(soup):\n",
    "    \"\"\" Returns all the races that took place in a particular season\n",
    "        for which the soup was passed in \"\"\"\n",
    "    find = soup.find(id='event')\n",
    "    if find is None:\n",
    "        r = []\n",
    "    else:\n",
    "        r = find.find_all('option')\n",
    "    return r\n",
    "\n",
    "def get_all_cats(soup):\n",
    "    \"\"\" Returns all the different categories (MotoGP, Moto2, etc.)\n",
    "        that took place at a particular track in the provided soup \"\"\"\n",
    "    find = soup.find(id='category')\n",
    "    if find is None:\n",
    "        r = []\n",
    "    else:\n",
    "        r = find.find_all('option')\n",
    "    return r\n",
    "\n",
    "def get_race_sessions(soup):\n",
    "    \"\"\" Returns all the different race sessions (RACE, RACE2, etc.)\n",
    "        that took place at a particular track in the provided soup \"\"\"\n",
    "    find = soup.find(id='session')\n",
    "    r = []\n",
    "    if find is None:\n",
    "        return r\n",
    "    else:\n",
    "        r2 = find.find_all('option')\n",
    "        for s in r2:\n",
    "            if s.text.find('RACE') > -1:\n",
    "                r.append(s.text.replace('E',''))\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) function to get stats about all riders in a specific race\n",
    "def get_all_stats(soup, year, trk, track, cat, ssn):\n",
    "    \n",
    "    if soup.find('tbody') is None:\n",
    "        return [dict(zip(headers, [year, trk, track, cat, ssn]+['n/a']*(len(headers)-3)))]\n",
    "    else:\n",
    "        riders = soup.find('tbody').find_all('a')\n",
    "        stats_to_return = []\n",
    "\n",
    "        # raceday stats\n",
    "        date = get_date(soup)\n",
    "        tr_con = get_tr_con(soup)\n",
    "        tr_tmp = get_tr_tmp(soup)\n",
    "        air_tmp = get_air_tmp(soup)\n",
    "        humid = get_humidity(soup)\n",
    "\n",
    "        # rider stats\n",
    "        for r in riders:\n",
    "            pos = r.findPrevious().findPrevious().findPrevious().findPrevious().text\n",
    "            if pos=='':\n",
    "                pos='crash'\n",
    "            else:\n",
    "                pos=int(pos)    \n",
    "            points = r.findPrevious().findPrevious().findPrevious().text\n",
    "            if points=='':\n",
    "                points=0\n",
    "            else:\n",
    "                points=float(points)\n",
    "            r_num = r.findPrevious().findPrevious().text\n",
    "            if r_num != '':\n",
    "                r_num = int(r_num)\n",
    "            r_nam = r.text\n",
    "            r_nat = r.findNext().text\n",
    "            team = r.findNext().findNext().text\n",
    "            bike = r.findNext().findNext().findNext().text\n",
    "            avgspd = r.findNext().findNext().findNext().findNext().text\n",
    "            time = r.findNext().findNext().findNext().findNext().findNext().text\n",
    "\n",
    "            stats_dict = dict(zip(headers, [year, trk, track, cat, ssn, date, tr_con, tr_tmp, air_tmp,\n",
    "                                            humid, pos, points, r_num, r_nam, r_nat, team,\n",
    "                                            bike, avgspd, time]))\n",
    "            stats_to_return.append(stats_dict)\n",
    "\n",
    "        return stats_to_return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Scraping Race Results <a name='scraper'></a>\n",
    "The basic structure of the following scraper is this:\n",
    "* For each year of racing\n",
    "    * Get all racetracks MotoGP visited in that season\n",
    "    * For each racetrack\n",
    "        * Get all categories they raced in (MotoGP, Moto2, etc.)\n",
    "        * For each category\n",
    "            * Get all racing sessions (usually one RACE sessions per class, but sometimes two)\n",
    "            * For each racing session\n",
    "                * Parse table of results for each rider\n",
    "                * Get track conditions  \n",
    "                \n",
    "Once it loops through all the steps above, it converts the resulting list of dictionaries into a pandas dataframe, then saves it as a comma-separated values (CSV) file for that particular year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006\n",
      "SPA, QAT, TUR, CHN, FRA, ITA, CAT, NED, GBR, GER, USA, CZE, MAL, AUS, JPN, POR, VAL, 2006_data.csv\n",
      "2005\n",
      "SPA, POR, CHN, FRA, ITA, CAT, NED, USA, GBR, GER, CZE, JPN, MAL, QAT, AUS, TUR, VAL, 2005_data.csv\n",
      ">> Scraping complete!\n"
     ]
    }
   ],
   "source": [
    "# loop through all parameters\n",
    "\n",
    "for yr in reversed(years):\n",
    "    data_list = []\n",
    "    soup_yr = soup_special(base_url + yr)\n",
    "    races = get_all_races(soup_yr)\n",
    "    print(yr)\n",
    "    \n",
    "    for rc in races:\n",
    "        TRK = rc['value']\n",
    "        Track = rc['title']\n",
    "        print(TRK, end=\", \")\n",
    "        url_rc = base_url +yr +'/' +TRK +'/'\n",
    "        soup_rc = soup_special(url_rc)\n",
    "        categories = get_all_cats(soup_rc)\n",
    "        \n",
    "        for cat in categories:\n",
    "            CAT = cat.text\n",
    "            url_c = base_url +yr +'/' +TRK +'/' + CAT + '/'\n",
    "            soup_c = soup_special(url_c)\n",
    "            sessions = get_race_sessions(soup_c)\n",
    "            \n",
    "            for ssn in sessions:\n",
    "                SSN = ssn\n",
    "                url_ssn = base_url +yr +'/' +TRK +'/' + CAT + '/' + SSN + '/Classification'\n",
    "                soup_ssn = soup_special(url_ssn)\n",
    "                data_list.extend(get_all_stats(soup_ssn, yr, TRK, Track, CAT, SSN))\n",
    "                time.sleep(1+np.random.random())\n",
    "    \n",
    "    df = pd.DataFrame(data_list, columns=headers)\n",
    "    fn = '/Archive/' + yr + '_data.csv'\n",
    "    df.to_csv(fn)\n",
    "    print(fn)\n",
    "    time.sleep(1+np.random.random())\n",
    "\n",
    "print('>> Scraping complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scraping Racetrack Information <a name ='tracks'></a>  \n",
    "We'll use some of the scraper code above to get information about the racetracks too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2017 - QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, CZE, AUT, GBR, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2016 - QAT, ARG, AME, SPA, FRA, ITA, CAT, NED, GER, AUT, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2015 - QAT, AME, ARG, SPA, FRA, ITA, CAT, NED, GER, INP, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2014 - QAT, AME, ARG, SPA, FRA, ITA, CAT, NED, GER, INP, CZE, GBR, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2013 - QAT, AME, SPA, FRA, ITA, CAT, NED, GER, USA, INP, CZE, GBR, RSM, ARA, MAL, AUS, JPN, VAL, \n",
      "2012 - QAT, SPA, POR, FRA, CAT, GBR, NED, GER, ITA, USA, INP, CZE, RSM, ARA, JPN, MAL, AUS, VAL, \n",
      "2011 - QAT, SPA, POR, FRA, CAT, GBR, NED, ITA, GER, USA, CZE, INP, RSM, ARA, JPN, AUS, MAL, VAL, \n",
      "2010 - QAT, SPA, FRA, ITA, GBR, NED, CAT, GER, USA, CZE, INP, RSM, ARA, JPN, MAL, AUS, POR, VAL, \n",
      "2009 - QAT, JPN, SPA, FRA, ITA, CAT, NED, USA, GER, GBR, CZE, INP, RSM, POR, AUS, MAL, VAL, \n",
      "2008 - QAT, SPA, POR, CHN, FRA, ITA, CAT, GBR, NED, GER, USA, CZE, RSM, INP, JPN, AUS, MAL, VAL, \n",
      "2007 - QAT, SPA, TUR, CHN, FRA, ITA, CAT, GBR, NED, GER, USA, CZE, RSM, POR, JPN, AUS, MAL, VAL, \n",
      "2006 - SPA, QAT, TUR, CHN, FRA, ITA, CAT, NED, GBR, GER, USA, CZE, MAL, AUS, JPN, POR, VAL, \n",
      "2005 - SPA, POR, CHN, FRA, ITA, CAT, NED, USA, GBR, GER, CZE, JPN, MAL, QAT, AUS, TUR, VAL, "
     ]
    }
   ],
   "source": [
    "# first, get all tracks from 2005-2017\n",
    "track_list = []\n",
    "GPs_list = []\n",
    "track_names = []\n",
    "\n",
    "for yr in reversed(years):\n",
    "    soup_yr = soup_special(base_url + yr)\n",
    "    races = get_all_races(soup_yr)\n",
    "    print('')\n",
    "    print(yr, end = \" - \")\n",
    "    \n",
    "    for rc in races:\n",
    "        TRK = rc['value']\n",
    "        Track = rc['title']\n",
    "        print(TRK, end=\", \")\n",
    "        track_list.append(TRK)\n",
    "        GPs_list.append(Track.split(' - ')[0])\n",
    "        track_names.append(Track.split(' - ')[1])\n",
    "        \n",
    "    time.sleep(1+np.random.random())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the unique ones \n",
    "combined_list = []\n",
    "for index, item in enumerate(track_list):\n",
    "    combined_list.append(item+' - '+track_names[index])\n",
    "combined_track_set = set(combined_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next part requires a bit of manual work, since there is no simple way to get from the track names to the URLs with their information. Additionally, several of the tracks from the earlier years no longer have information available on the MotoGP website in an HTML page, so that is manually added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_track_set\n",
    "track_url_dict = {'AME - Circuit Of The Americas':'Americas',\n",
    "                  'ARA - MotorLand Aragón':'Aragon',\n",
    "                  'ARG - Termas de Río Hondo':'Argentina',\n",
    "                  'AUS - Phillip Island':'Australia',\n",
    "                  'AUT - Red Bull Ring – Spielberg':'Austria',\n",
    "                  'CAT - Circuit de Barcelona-Catalunya':'Catalunya',\n",
    "                  'CHN - Shanghai Circuit':0,\n",
    "                  'CZE - Automotodrom Brno':'Czech+Republic',\n",
    "                  'FRA - Le Mans':'France',\n",
    "                  'GBR - Donington Park Circuit':0,\n",
    "                  'GBR - Silverstone Circuit':'Great+Britain',\n",
    "                  'GER - Sachsenring':'Germany',\n",
    "                  'INP - Indianapolis Motor Speedway':0,\n",
    "                  'ITA - Autodromo del Mugello':'Italy',\n",
    "                  'JPN - Twin Ring Motegi':'Japan',\n",
    "                  'MAL - Sepang International Circuit':'Malaysia',\n",
    "                  'NED - TT Circuit Assen':'Netherlands',\n",
    "                  'POR - Estoril Circuit':0,\n",
    "                  'QAT - Losail International Circuit':'Qatar',\n",
    "                  'RSM - Misano World Circuit Marco Simoncelli':'San+Marino',\n",
    "                  'SPA - Circuito de Jerez':'Spain',\n",
    "                  'TUR - Istanbul Circuit':0,\n",
    "                  'USA - Mazda Raceway Laguna Seca':0,\n",
    "                  'VAL - Circuit Ricardo Tormo':'Valencia'    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to get basic track info\n",
    "def get_GP_info(track_url_str):\n",
    "    \"\"\"\n",
    "    Returns a list with track length, number of left corners, number of right corners,\n",
    "    track width, and length of longest straight. For any unavailable values, it returns\n",
    "    'n/a' instead of a float or int.\n",
    "    \"\"\"\n",
    "    url = 'http://www.motogp.com/en/event/' + track_url_str + '#info-track'\n",
    "    soupy = soup_special(url)\n",
    "    attributes = soupy.find(id='circuit_numbers').find_all(class_='circuit_number_content')\n",
    "    strs = []\n",
    "    list_data = []\n",
    "    \n",
    "    for s in range(len(attributes)):\n",
    "        strs.append(attributes[s].text)\n",
    "\n",
    "    if float(strs[0].split()[0])==0:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[0].split()[0]))\n",
    "\n",
    "    if strs[1]=='':\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(int(strs[1]))\n",
    "    \n",
    "    if strs[2]=='':\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(int(strs[2]))\n",
    "    \n",
    "    if len(strs[3].split())==1:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[3].split()[0]))\n",
    "    \n",
    "    if len(strs[4].split())==1:\n",
    "        list_data.append('n/a')\n",
    "    else:\n",
    "        list_data.append(float(strs[4].split()[0]))\n",
    "\n",
    "    return list_data\n",
    "\n",
    "def get_GP_info_additional(track_url_str):\n",
    "    \"\"\"\n",
    "    Returns MotoGP average speed, MotoGP distance, Moto2 distance,\n",
    "    and Moto3 distance for the particular track. If data does not exist,\n",
    "    it returns 'n/a' in place of a float or int.\n",
    "    \"\"\"\n",
    "    url = 'http://www.motogp.com/en/event/' + track_url_str + '#info-track'\n",
    "    soupy = soup_special(url)\n",
    "    \n",
    "    # MotoGP average speed\n",
    "    avg_speed_str = soupy.find(class_='c-statistics__speed-item').text\n",
    "    if avg_speed_str == '-':\n",
    "        avg_speed = 'n/a'\n",
    "    else:\n",
    "        avg_speed = float(avg_speed_str)\n",
    "    \n",
    "    attributes = soupy.find(class_='c-laps__content').find_all(class_='c-laps__item')\n",
    "    \n",
    "    # MotoGP distance\n",
    "    GP_dist = float(attributes[9].text.split()[0])\n",
    "    if GP_dist==0: GP_dist='n/a'\n",
    "        \n",
    "    # Moto2 distance\n",
    "    m2_dist = float(attributes[10].text.split()[0])\n",
    "    if m2_dist==0: m2_dist='n/a'\n",
    "        \n",
    "    # Moto3 distance\n",
    "    m3_dist = float(attributes[11].text.split()[0])\n",
    "    if m3_dist==0: m3_dist='n/a'\n",
    "        \n",
    "    return [avg_speed, GP_dist, m2_dist, m3_dist] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "////////////////////////////////////Complete!\n"
     ]
    }
   ],
   "source": [
    "# make a list of dictionaries for track information\n",
    "headers_2 = ['GP','track_length_km','l_corners','r_corners',\n",
    "           'width_m','straight_m','GP_avg_speed','gp_dist',\n",
    "           'm2_dist','m3_dist']\n",
    "\n",
    "track_data = []\n",
    "for track in combined_track_set:\n",
    "    if track_url_dict[track] != 0:\n",
    "        print('//', end='')\n",
    "        l_GP, L_c, R_c, wid, strt = get_GP_info(track_url_dict[track])\n",
    "        GP_avg_spd, gp_d, m2_d, m3_d = get_GP_info_additional(track_url_dict[track])\n",
    "        track_dict = dict(zip(headers, [track,l_GP,L_c,R_c,wid,strt,GP_avg_spd,gp_d,m2_d,m3_d]))\n",
    "        track_data.append(track_dict)\n",
    "        time.sleep(1+np.random.random())\n",
    "print('Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually add in the info for tracks which have a 0 in the track_url_dict\n",
    "# information is from archived PDFs like the one at this following link\n",
    "# http://resources.motogp.com/files/results/2006/CHN/circuit+information.pdf?v1_96143780\n",
    "\n",
    "dict_shanghai = {'GP': 'CHN - Shanghai Circuit','GP_avg_speed': 'n/a','gp_dist': 'n/a',\n",
    "                 'l_corners': 7,'m2_dist': 'n/a','m3_dist': 'n/a','r_corners': 7,\n",
    "                 'straight_m': 1202.0,'track_length_km': 5.281,'width_m': 14.0}\n",
    "\n",
    "dict_donington = {'GP': 'GBR - Donington Park Circuit','GP_avg_speed': 'n/a','gp_dist': 'n/a',\n",
    "                  'l_corners': 4,'m2_dist': 'n/a','m3_dist': 'n/a','r_corners': 7,\n",
    "                  'straight_m': 564.0,'track_length_km': 4.023,'width_m': 10.0}\n",
    "\n",
    "dict_indianapolis = {'GP': 'INP - Indianapolis Motor Speedway','GP_avg_speed': 'n/a','gp_dist': 'n/a',\n",
    "                     'l_corners': 10,'m2_dist': 'n/a','m3_dist': 'n/a','r_corners': 6,\n",
    "                     'straight_m': 644.0,'track_length_km': 4.216,'width_m': 16.0}\n",
    "\n",
    "dict_estoril = {'GP': 'POR - Estoril Circuit','GP_avg_speed': 'n/a','gp_dist': 'n/a',\n",
    "                'l_corners': 4,'m2_dist': 'n/a','m3_dist': 'n/a','r_corners': 9,\n",
    "                'straight_m': 986.0,'track_length_km': 4.182,'width_m': 14.0}\n",
    "\n",
    "dict_istanbul = {'GP': 'TUR - Istanbul Circuit','GP_avg_speed': 'n/a','gp_dist': 'n/a',\n",
    "                 'l_corners': 8,'m2_dist': 'n/a','m3_dist': 'n/a','r_corners': 6,\n",
    "                 'straight_m': 720.0,'track_length_km': 5.340,'width_m': 21.0}\n",
    "\n",
    "dict_laguna = {'GP': 'USA - Mazda Raceway Laguna Seca','GP_avg_speed': 'n/a','gp_dist': 'n/a',\n",
    "                  'l_corners': 7,'m2_dist': 'n/a','m3_dist': 'n/a','r_corners': 4,\n",
    "                  'straight_m': 966.0,'track_length_km': 3.610,'width_m': 15.0}\n",
    "\n",
    "track_data.append(dict_shanghai)\n",
    "track_data.append(dict_donington)\n",
    "track_data.append(dict_indianapolis)\n",
    "track_data.append(dict_estoril)\n",
    "track_data.append(dict_istanbul)\n",
    "track_data.append(dict_laguna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Racetrack_data.csv\n"
     ]
    }
   ],
   "source": [
    "# save to CSV\n",
    "df_tracks = pd.DataFrame(track_data, columns=headers_2)\n",
    "fn = 'Racetrack_data.csv'\n",
    "df_tracks.to_csv(fn)\n",
    "print(fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
